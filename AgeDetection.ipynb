{"cells":[{"cell_type":"code","metadata":{"source_hash":"788c210e","execution_start":1706732612182,"execution_millis":505,"deepnote_to_be_reexecuted":false,"cell_id":"45883e8e7c7249f3bdddb34fb37bb894","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport os\nimport numpy as np\nfrom tabulate import tabulate\nimport dlib\nimport cv2\nfrom tqdm import tqdm\nimport zipfile\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom skimage import filters\nfrom tqdm import tqdm\nimport plotly.express as px","block_group":"905a9fe6e201466abdcd7801235da4ed","execution_count":2,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'dlib'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtabulate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"]}]},{"cell_type":"code","metadata":{"source_hash":"9796a15c","execution_start":1706732613406,"execution_millis":28,"deepnote_to_be_reexecuted":false,"cell_id":"6f1d48accc0d4459b2ab6ad2b28237a6","deepnote_cell_type":"code"},"source":"zip_path = r\"C:\\Users\\LEGION\\Downloads\\Agedetection\\age_detection.zip\"  # Update this with the correct path\n\n# Unzip the file\nwith zipfile.ZipFile(zip_path, 'r') as zip_ref:\n    zip_ref.extractall()\n\n# Read the CSV file\ndf = pd.read_csv('age_detection.csv')\n\n# Display the first few rows of the dataframe\nprint(df.head())\n\n# Get the file path of the first image\nimage_path = df.loc[0, 'file']\n\n# Check if the image path does not contain the absolute path, if not prepend the path\nif not os.path.isabs(image_path):\n    image_path = os.path.join(os.path.dirname(zip_path), image_path)\n\n# Open and display the image\ntry:\n    img = Image.open(image_path)\n    img.show()\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n","block_group":"6f1d48accc0d4459b2ab6ad2b28237a6","execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'zipfile' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m zip_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLEGION\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAgedetection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mage_detection.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update this with the correct path\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Unzip the file\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241m.\u001b[39mZipFile(zip_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      5\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Read the CSV file\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"78b923a6","execution_start":1706732614361,"execution_millis":8,"deepnote_to_be_reexecuted":false,"cell_id":"0cacf2d1f66242fe8643b15ce2d77364","deepnote_cell_type":"code"},"source":"print(df.columns) \nprint(df.shape)  ","block_group":"40f71b4df96945f6afa4f83abd299c32","execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns) \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"9b485d0ca6364612b99f5e03757cbf09","deepnote_cell_type":"text-cell-h1"},"source":"# Data Cleaning","block_group":"1a69904c9580489b808e249421c2159d"},{"cell_type":"code","metadata":{"source_hash":"db21dc56","execution_start":1706732615994,"execution_millis":25,"deepnote_to_be_reexecuted":false,"cell_id":"3ffa2602370f4ab4bf3303b02aa734c5","deepnote_cell_type":"code"},"source":"# File Path Validation\ndf = df[df['file'].apply(lambda x: os.path.isfile(x))]\n\n# Duplicate Removal\ndf.drop_duplicates(subset=['file'], keep='first', inplace=True)","block_group":"fa09ab247e3042bcbfad8712b93ce1e1","execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File Path Validation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(x))]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Duplicate Removal\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m], keep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"2e837660","execution_start":1706732617055,"execution_millis":9,"deepnote_to_be_reexecuted":false,"cell_id":"77149df184ee4a06931f0b8c9f0b51bb","deepnote_cell_type":"code"},"source":"# Split Consistency\ndf = df[df['split'].isin(['train', 'test'])]","block_group":"cfa4f9301d884630811de3ea7d81e115","execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split Consistency\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"79a6c41","execution_start":1706732617844,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"386ff085710241dcaee40c35848d3e18","deepnote_cell_type":"code"},"source":"# Image Quality\n# removin images that are too small\nmin_width, min_height = 64, 64  # minimum acceptable dimensions\ndef is_image_large_enough(file_path):\n    with Image.open(file_path) as img:\n        return img.width >= min_width and img.height >= min_height\ndf = df[df['file'].apply(is_image_large_enough)]\n\n# Image Preprocessing\n# resizing and normalizing images\ndef preprocess_image(file_path):\n    with Image.open(file_path) as img:\n        img = img.resize((min_width, min_height))  # resize\n        img = img.convert('RGB')  # ensure 3 channels\n        img = np.array(img) / 255.0  # normalize to [0, 1]\n    return img\ndf['image'] = df['file'].apply(preprocess_image)\n","block_group":"8e549723c60f4df98eda6e6a9d5b7d76","execution_count":7,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mopen(file_path) \u001b[38;5;28;01mas\u001b[39;00m img:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_width \u001b[38;5;129;01mand\u001b[39;00m img\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_height\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(is_image_large_enough)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Image Preprocessing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# resizing and normalizing images\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_image\u001b[39m(file_path):\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"78b923a6","execution_start":1706732618639,"execution_millis":11,"deepnote_to_be_reexecuted":false,"cell_id":"98600e6348b347a6bb3a96ad797beaaf","deepnote_cell_type":"code"},"source":"print(df.columns) \nprint(df.shape)  ","block_group":"eb0005f6a82d49568ecc6e4389c27d7a","execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns) \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mshape)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"5210a8bb","execution_start":1706732619348,"execution_millis":27,"deepnote_to_be_reexecuted":false,"cell_id":"0a27ad85f3b046749bde092c7c05e2d9","deepnote_cell_type":"code"},"source":"#Face Detection\ndetector = dlib.get_frontal_face_detector()\n\ndef contains_face(file_path):\n    img = cv2.imread(file_path)\n    faces = detector(img, 1)\n    return len(faces) > 0\n\ndf = df[df['file'].apply(contains_face)]","block_group":"b2994cbcd72b487181960dcef651fc88","execution_count":9,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dlib' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Face Detection\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_face\u001b[39m(file_path):\n\u001b[1;32m      5\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path)\n","\u001b[0;31mNameError\u001b[0m: name 'dlib' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"cb56d897","execution_start":1706732620139,"execution_millis":6790,"deepnote_to_be_reexecuted":false,"cell_id":"8ed239e0c4f44106b27dfd5f632687b1","deepnote_cell_type":"code"},"source":"import urllib.request\nimport bz2\n\n# URL of the file to be downloaded\nurl = \"http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\"\n\n# Path where the downloaded file will be stored\noutput_path = \"shape_predictor_68_face_landmarks.dat.bz2\"\n\n# Download the file from `url` and save it locally under `output_path`:\nurllib.request.urlretrieve(url, output_path)\n\n# Open the .bz2 file for reading\nwith bz2.open(output_path, 'rb') as f:\n    # Decompress the data\n    decompressed_data = f.read()\n\n# Write the decompressed data to a new file\nwith open('shape_predictor_68_face_landmarks.dat', 'wb') as f:\n    f.write(decompressed_data)\n","block_group":"0e05f72cdabf42ce95e89273edaa4029","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"53a56971","execution_start":1706732626949,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"340b2e1543ee4f7c8ef140cd5edecec8","deepnote_cell_type":"code"},"source":"# Face Alignment\npredictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  # Download this file\n\ndef align_face(file_path):\n    img = cv2.imread(file_path)\n    faces = detector(img, 1)\n    for rect in faces:\n        shape = predictor(img, rect)\n        aligned_face = dlib.get_face_chip(img, shape)\n        return aligned_face\n\ndf['aligned_face'] = df['file'].apply(align_face)\n\n# Background Removal\n\n# Lighting and Color Normalization\ndef normalize_image(face):\n    gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n    normalized = cv2.equalizeHist(gray)\n    return normalized\n\ndf['normalized_face'] = df['aligned_face'].apply(normalize_image)\n\n#Data Augmentation\ndef augment_image(face):\n    M = cv2.getRotationMatrix2D((face.shape[1] / 2, face.shape[0] / 2), np.random.uniform(-30, 30), 1)\n    rotated = cv2.warpAffine(face, M, (face.shape[1], face.shape[0]))\n    return rotated\n\ndf['augmented_face'] = df['normalized_face'].apply(augment_image)","block_group":"6ddeab1a6cab499fa745d077a2c36c71","execution_count":11,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dlib' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Face Alignment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241m.\u001b[39mshape_predictor(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Download this file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21malign_face\u001b[39m(file_path):\n\u001b[1;32m      5\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(file_path)\n","\u001b[0;31mNameError\u001b[0m: name 'dlib' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"deb38141","deepnote_to_be_reexecuted":true,"cell_id":"11398b9a3df749f98c72211a632970ba","deepnote_cell_type":"code"},"source":"Eencoding age\nlabel_encoder = LabelEncoder()\ndf['ageLabel'] = label_encoder.fit_transform(df['age'])\ndf['ageLabel']","block_group":"eb190e3ab712406099a014949b64983a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"5491fb6b","deepnote_to_be_reexecuted":true,"cell_id":"a5b305ad3e1a44de86ded22d43ef9008","deepnote_cell_type":"code"},"source":"def display_images(images, labels, num_images=5):\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n    \n    for i in range(num_images):\n        axes[i].imshow(images[i])\n        axes[i].set_title(f'Label: {labels[i]}')\n        axes[i].axis('off')\n    \n    plt.show()\n\n# Display the first 5 images from the training set\ndisplay_images(df['augmented_face'][:5], label_encoder.inverse_transform(df['ageLabel'])[:5])\n","block_group":"db14ad7cc40b491fb3de2021c459c6de","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1d801dd8159f4a92a8d0fbe56cec697a","deepnote_cell_type":"text-cell-h1"},"source":"# EDA and feature engineering","block_group":"2019cc31017141618e0be715b8cfa091"},{"cell_type":"code","metadata":{"source_hash":"792fa0cb","execution_start":1706513058840,"execution_millis":1611,"deepnote_to_be_reexecuted":true,"cell_id":"2ad8e12927564acda101c441b520d61b","deepnote_cell_type":"code"},"source":"#plotting the age and their disturbution\n\n\nage_counts = df['label'].value_counts()\n\n# Plotting\nplt.figure(figsize=(12, 6))\nage_counts.plot(kind='bar', color='skyblue')\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.xticks(rotation=45)\nplt.show()","block_group":"919e124c9e4d4f8197e431d0deb95b27","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Assuming 'age' is a column in your DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Replace 'your_dataframe' with the actual name of your DataFrame\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m age_counts \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Plotting\u001b[39;00m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"7c289a79","deepnote_to_be_reexecuted":true,"cell_id":"edb0e3420ab24f2cb5b381bfdcc5bd83","deepnote_cell_type":"code"},"source":"age_counts = df['age'].value_counts()\nage_counts.index\nage_counts.values\nage_counts_df = pd.DataFrame({'age':age_counts.index,'Counts':age_counts.values})\npx.bar(data_frame=age_counts_df,\n x='age',\n y='Counts',\n color='Counts',\n color_continuous_scale='blues',\n text_auto=True,\n title=f'Age Distribution')","block_group":"44b967712e1a4f158d84599fbd9b63f2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"e3ccee2d","execution_start":1706728035866,"execution_millis":2282,"deepnote_to_be_reexecuted":true,"cell_id":"4860d8d47a4e4bc294043f5fab62fcfb","deepnote_cell_type":"code"},"source":"#showing images\nimport matplotlib.pyplot as plt\n\n# Function to display images\ndef display_images(images, labels, num_images=5):\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 3))\n    \n    for i in range(num_images):\n        axes[i].imshow(images[i])\n        axes[i].set_title(f'Label: {labels[i]}')\n        axes[i].axis('off')\n    \n    plt.show()\n\n# Display the first 5 images from the training set\ndisplay_images(df['augmented_face'][:5], label_encoder.inverse_transform(df['ageLabel'])[:5])\n","block_group":"68bdd983bbb14e19a37babb703ac6c27","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Display the first 5 images from the training set\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m display_images(\u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugmented_face\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m5\u001b[39m], label_encoder\u001b[38;5;241m.\u001b[39minverse_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mageLabel\u001b[39m\u001b[38;5;124m'\u001b[39m])[:\u001b[38;5;241m5\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"1fc367c4","execution_start":1706524243699,"execution_millis":1909,"deepnote_to_be_reexecuted":true,"cell_id":"3520b8446ae94fdf9286da26ee8976e6","deepnote_cell_type":"code"},"source":"#Calculate mean and standard deviation along each channel\n\nimage_array = np.array(train_images)\n\nmean_values = np.mean(image_array, axis=(0, 1, 2))\nstd_values = np.std(image_array, axis=(0, 1, 2))\n\n# Create a DataFrame for better presentation\nstatistics_df = pd.DataFrame({\n    'Channel': ['Red', 'Green', 'Blue'],\n    'Mean': mean_values,\n    'Standard Deviation': std_values\n})\n\n# Display the DataFrame with a styled format\nstyled_df = statistics_df.style.format({\n    'Mean': '{:.2f}',\n    'Standard Deviation': '{:.2f}'\n})\nstyled_df","block_group":"f1a9ffb43c024116b26f1a7e0669c700","execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'np' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image_array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(train_images)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate mean and standard deviation along each channel\u001b[39;00m\n\u001b[1;32m      5\u001b[0m mean_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(image_array, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"8dc363825a2a45f2ac0872da4ddc1f82","deepnote_cell_type":"text-cell-h3"},"source":"### skin texture","block_group":"5568186f497145b29b0f8a11152b012c"},{"cell_type":"code","metadata":{"source_hash":"89fc0a89","execution_start":1706732628035,"execution_millis":15,"deepnote_to_be_reexecuted":false,"cell_id":"2d0613ec18564c9986593fe9fac3a315","deepnote_cell_type":"code"},"source":"# Create a new column in the DataFrame to store texture features\ndf['texture_features'] = None\n\n# Function to extract basic texture features from an image\ndef get_texture_features(image_path):\n    # Load the image\n    image = cv2.imread(image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply Gaussian blur to smooth the image\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # Compute texture features using the Laplacian operator\n    laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n    laplacian_var = np.var(laplacian)\n\n    return laplacian_var\n\n# Apply the get_texture_features function to each image in the DataFrame\ntexture_features_list = []\nfor image_path in tqdm(train_df['filepaths']):\n    texture_features = get_texture_features(image_path)\n    texture_features_list.append(texture_features)\n\n# Add the extracted texture features to the DataFrame\ndf['texture_features'] = texture_features_list\n\n# Visualize Texture Features Across Age Groups\nage_groups = df.groupby('label')\n\n# Set up subplots for visualization\nfig, axs = plt.subplots(len(age_groups), figsize=(8, 5 * len(age_groups)))\n\nfor i, (age_group, group_data) in enumerate(age_groups):\n    axs[i].set_title(f\"Age Group: {age_group}\")\n\n    # Visualize texture features for a sample of images in the age group\n    axs[i].hist(group_data['texture_features'], bins=20, color='skyblue', edgecolor='black')\n    axs[i].set_xlabel('Texture Features')\n    axs[i].set_ylabel('Frequency')\n\nplt.show()\n","block_group":"12ceccbba11948f2ab4401ea460058e2","execution_count":12,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a new column in the DataFrame to store texture features\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexture_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to extract basic texture features from an image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_texture_features\u001b[39m(image_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the image\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"fa26cac6d389439890d43ef06327238b","deepnote_cell_type":"text-cell-h3"},"source":"### wrinkle","block_group":"a97e56e97322456baf34ab0a1ac408a2"},{"cell_type":"code","metadata":{"source_hash":"7b942226","execution_start":1706732629548,"execution_millis":15,"deepnote_to_be_reexecuted":false,"cell_id":"5546df3de13c4cae8188ac1b87c397d1","deepnote_cell_type":"code"},"source":"# Create a new column in the DataFrame to store wrinkle features\ndf['wrinkle_features'] = None\n\n# Function to extract basic wrinkle features from an image\ndef get_wrinkle_features(image_path):\n    # Load the image\n    image = cv2.imread(image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    # Apply Canny edge detection to enhance wrinkles\n    edges = cv2.Canny(gray, 50, 150)\n\n    # Compute the percentage of white pixels in the edges\n    wrinkle_percentage = np.sum(edges) / (gray.shape[0] * gray.shape[1])\n\n    return wrinkle_percentage\n\n# Apply the get_wrinkle_features function to each image in the DataFrame\nwrinkle_features_list = []\nfor image_path in tqdm(train_df['filepaths']):\n    wrinkle_features = get_wrinkle_features(image_path)\n    wrinkle_features_list.append(wrinkle_features)\n\n# Add the extracted wrinkle features to the DataFrame\ndf['wrinkle_features'] = wrinkle_features_list\n\n# Visualize Wrinkle Features Across Age Groups\nage_groups = df.groupby('label')\n\n# Set up subplots for visualization\nfig, axs = plt.subplots(len(age_groups), figsize=(8, 5 * len(age_groups)))\n\nfor i, (age_group, group_data) in enumerate(age_groups):\n    axs[i].set_title(f\"Age Group: {age_group}\")\n\n    # Visualize wrinkle features for a sample of images in the age group\n    axs[i].hist(group_data['wrinkle_features'], bins=20, color='lightcoral', edgecolor='black')\n    axs[i].set_xlabel('Wrinkle Features')\n    axs[i].set_ylabel('Frequency')\n\nplt.show()","block_group":"957d66b637da4b6895e213d99147b6f3","execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a new column in the DataFrame to store wrinkle features\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrinkle_features\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to extract basic wrinkle features from an image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wrinkle_features\u001b[39m(image_path):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load the image\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"23687019cba64ef3b71233c96780a413","deepnote_cell_type":"text-cell-h3"},"source":"### hair color","block_group":"44d4a9d81083458992c92236ad1fe90b"},{"cell_type":"code","metadata":{"source_hash":"3be70e95","execution_start":1706732631267,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"3f407a4a967c45ca96dbff5ee2aabb8c","deepnote_cell_type":"code"},"source":"#probably useless\n\ndef extract_hair_color(image_path):\n    # Load the image\n    image = cv2.imread(image_path)\n    # Convert the image to HSV color space for better color analysis\n    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # Define a mask for the hair region in the HSV color space\n    lower_hair_color = np.array([0, 10, 40])\n    upper_hair_color = np.array([30, 200, 255])\n    hair_mask = cv2.inRange(hsv_image, lower_hair_color, upper_hair_color)\n    \n    # Apply the hair mask to the original image\n    hair_region = cv2.bitwise_and(image, image, mask=hair_mask)\n    \n    # Calculate the dominant hair color\n    dominant_color = np.mean(hair_region, axis=(0, 1)).astype(int)\n    \n    return dominant_color\n\n# Apply the extract_hair_color function to each image in the DataFrame\ndf['hair_color'] = df['filepaths'].apply(extract_hair_color)\n","block_group":"0a67e208f2d647e49b0dbc29d2b6301a","execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'df' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [14], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dominant_color\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Apply the extract_hair_color function to each image in the DataFrame\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhair_color\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepaths\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_hair_color)\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"0f6f85f193f2439b9e9d814fc1a31dd9","deepnote_cell_type":"text-cell-h2"},"source":"## facial landmarks","block_group":"fbf85acc324240d3bc0db7cc2262d6a9"},{"cell_type":"code","metadata":{"source_hash":"f5c99d88","execution_start":1706732637785,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"73ac6fb50ade4cf28a5241eba54062a7","deepnote_cell_type":"code"},"source":"\n\n# Load the pre-trained facial landmark predictor\npredictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\n# Function to detect facial landmarks in an image\ndef detect_landmarks(image_pixels):\n    # Convert the list of pixel values to a NumPy array\n    image_array = np.array(image_pixels, dtype=np.uint8)\n\n    # Detect faces in the image\n    faces = detector(image_array)\n\n    # Loop over each face and get facial landmarks\n    landmarks_list = []\n    for face in faces:\n        shape = predictor(image_array, face)\n        landmarks = [(shape.part(i).x, shape.part(i).y) for i in range(shape.num_parts)]\n        landmarks_list.append(landmarks)\n\n        # Draw landmarks on the image (comment out if you don't want to display)\n        for (x, y) in landmarks:\n            cv2.circle(image_array, (x, y), 2, (0, 255, 0), -1)\n\n    # Display the image with landmarks (comment out if you don't want to display)\n    cv2.imshow(\"Facial Landmarks\", image_array)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n\n    return landmarks_list\n\n# Apply the detect_landmarks function to each image in the DataFrame\ndf['landmarks'] = df['augmented_face'].apply(detect_landmarks)\n\n# Now df['landmarks'] contains the detected facial landmarks for each image\n","block_group":"902775eb2b8f48a497f14b3faab8fa54","execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dlib' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the pre-trained facial landmark predictor\u001b[39;00m\n\u001b[1;32m      2\u001b[0m predictor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlib/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to the shape predictor model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m      4\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(predictor_path)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Function to detect facial landmarks in an image\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'dlib' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"360dcf22","execution_start":1706732643355,"execution_millis":28,"deepnote_to_be_reexecuted":false,"cell_id":"360e46742c8f4e129faa7e943ae57fab","deepnote_cell_type":"code"},"source":"#eye openness detection \n\n# Load the pre-trained facial landmark predictor\npredictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor(predictor_path)\n\n# Function to detect facial landmarks and extract eye-related features\ndef detect_eye_features(image_pixels):\n    # Convert the list of pixel values to a NumPy array\n    image_array = np.array(image_pixels, dtype=np.uint8)\n\n    # Ensure the image has 3 channels (for compatibility with cv2.COLOR_BGR2GRAY)\n    if image_array.ndim == 2:\n        image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n\n    # Detect faces in the image\n    faces = detector(gray)\n\n    # Check if a face is detected\n    if len(faces) > 0:\n        # Get facial landmarks for the first detected face\n        shape = predictor(gray, faces[0])\n\n        # Extract eye-related features\n        left_eye_openness = shape.part(47).y - shape.part(43).y  # Example: vertical distance between eyebrow and lower eyelid\n        right_eye_openness = shape.part(40).y - shape.part(38).y\n\n       \n\n    return left_eye_openness, right_eye_openness\n\n    return None\n\n# Example: Apply the detect_eye_features function to each image in the DataFrame\neye_features = df['normalized_face'].apply(detect_eye_features)\n\n# Example: Add the extracted eye features to the DataFrame\ndf[['left_eye_openness', 'right_eye_openness']] = pd.DataFrame(eye_features.tolist(), index=df.index)\n","block_group":"1dc749bc500c484480e39fb563d16137","execution_count":16,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'dlib' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#eye openness detection (im not sure how accurate this is and if it works)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the pre-trained facial landmark predictor\u001b[39;00m\n\u001b[1;32m      4\u001b[0m predictor_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdlib/shape_predictor_68_face_landmarks.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with the path to the shape predictor model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mdlib\u001b[49m\u001b[38;5;241m.\u001b[39mget_frontal_face_detector()\n\u001b[1;32m      6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m dlib\u001b[38;5;241m.\u001b[39mshape_predictor(predictor_path)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Function to detect facial landmarks and extract eye-related features\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'dlib' is not defined"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"65f61800f1f64ebbae71c80fbe4765de","deepnote_cell_type":"text-cell-h1"},"source":"# Model","block_group":"cb0347a7d33d440eab0bfc0680035dc7"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"02445ac522364f00b361023dd5916fe1","deepnote_cell_type":"text-cell-h3"},"source":"### cnn","block_group":"e483ae0a7c704bd8bdc79d4d232c2add"},{"cell_type":"code","metadata":{"cell_id":"588249328a0b4760baededdd4682bb12","deepnote_cell_type":"code"},"source":"df= df.dropna()","block_group":"6f08f8a48c9d4a19b2e68ede6a182659","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"bbca52a4","execution_start":1706732652528,"execution_millis":18,"deepnote_to_be_reexecuted":false,"cell_id":"1299ef86c8834ac2aa6d0f3660ba98e1","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\n\n# Assuming 'df' is your DataFrame\n\n# Constants for image preprocessing\ndesired_width = 128\ndesired_height = 128\nnum_channels = 3  # Assuming color images, adjust if grayscale\n\n# Step 1: Prepare Data\n# Extract 'eye openness', 'texture features', and 'wrinkle features'\ntabular_features = df_cleaned[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n# Standardize numerical features\nscaler = StandardScaler()\ntabular_features = scaler.fit_transform(tabular_features)\n\n# Load and preprocess images\ndesired_channels = 3  # Assuming color images, adjust if grayscale\n\ndef preprocess_image(image):\n    # Resize to a common size\n    resized_image = cv2.resize(image, (desired_width, desired_height))\n    # Ensure the image has the correct number of channels\n    if resized_image.shape[-1] != desired_channels:\n        # If the image has more than 3 channels, convert to grayscale\n        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n        # Add an extra dimension to represent the single channel\n        resized_image = np.expand_dims(resized_image, axis=-1)\n    # Normalize pixel values to be between 0 and 1\n    resized_image = resized_image / 255.0\n    # Add other preprocessing steps as needed (e.g., data augmentation)\n    return resized_image\n\nX_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n# Extract target\ny_target = df_cleaned['ageLabel'].values\n\n# Split data into training and testing sets\nX_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n    X_images, tabular_features, y_target, test_size=0.2, random_state=42\n)\n\n# Step 2: Build Models\n# Build CNN model for image feature extraction\nimage_input = Input(shape=(desired_width, desired_height, num_channels))\nx = Conv2D(32, (3, 3), activation='relu')(image_input)\nx = MaxPooling2D((2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D((2, 2))(x)\nx = Flatten()(x)\nimage_output = Dense(128, activation='relu')(x)\n\n# Build model for tabular features\ntabular_input = Input(shape=(X_tabular_train.shape[1],))\ntabular_output = Dense(128, activation='relu')(tabular_input)\n\n# Step 3: Combine Models\nconcatenated = Concatenate(axis=-1)([image_output, tabular_output])\nx = Dense(64, activation='relu')(concatenated)\noutput = Dense(1, activation='linear')(x)\n\ncombined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n\n# Step 4: Train and Evaluate\ncombined_model.compile(optimizer='adam', loss='mean_squared_error')\ncombined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\ny_pred = combined_model.predict([X_images_test, X_tabular_test])\n\n# Calculate metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n# Assuming you have binary classification labels for f1_score\ny_test_binary = (y_test > 0).astype(int)\ny_pred_binary = (y_pred > 0).astype(int)\nf1 = f1_score(y_test_binary, y_pred_binary)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'F1 Score: {f1}')\n","block_group":"9414ac2ad0ca40069ea8ac02e9a27344","execution_count":18,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cv2'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"eab26478c4594aeaa973cf35326a4c9a","deepnote_cell_type":"text-cell-p"},"source":"Mean Absolute Error: 1.2200337251027424\r\nMean Squared Error: 2.036018026809009\r\nF1 Score: 0.8679245283018869","block_group":"2ab34d694727441680fd78f1b5cea5c5"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"941c346dffc9424483a989a742250c3d","deepnote_cell_type":"text-cell-h3"},"source":"### vggface","block_group":"f341624d35b041fa82d46f44a198a958"},{"cell_type":"code","metadata":{"source_hash":"bbeef52f","execution_start":1706732658199,"execution_millis":76,"deepnote_to_be_reexecuted":false,"cell_id":"cb6ba54780914b73bcd7ffcf25e66c45","deepnote_cell_type":"code"},"source":"import pandas as pd\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input, Concatenate\nfrom keras_vggface.vggface import VGGFace\nfrom keras_vggface.utils import preprocess_input\n\n\n# Constants for image preprocessing\ndesired_width = 128\ndesired_height = 128\nnum_channels = 3  # Assuming color images, adjust if grayscale\n\n# Step 1: Prepare Data\n# Extract 'eye openness', 'texture features', and 'wrinkle features'\ntabular_features = df[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n# Standardize numerical features\nscaler = StandardScaler()\ntabular_features = scaler.fit_transform(tabular_features)\n\n# Load and preprocess images\ndesired_channels = 3  # Assuming color images, adjust if grayscale\n\ndef preprocess_image(image):\n    # Resize to a common size\n    resized_image = cv2.resize(image, (desired_width, desired_height))\n    # Ensure the image has the correct number of channels\n    if resized_image.shape[-1] != desired_channels:\n        # If the image has more than 3 channels, convert to grayscale\n        resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n        # Add an extra dimension to represent the single channel\n        resized_image = np.expand_dims(resized_image, axis=-1)\n    # Normalize pixel values to be between 0 and 1\n    resized_image = resized_image / 255.0\n    # Add other preprocessing steps as needed (e.g., data augmentation)\n    return resized_image\n\nX_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n# Extract target\ny_target = df['ageLabel'].values\n\n# Split data into training and testing sets\nX_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n    X_images, tabular_features, y_target, test_size=0.2, random_state=42\n)\n\n# Step 2: Build Models\n# Build VGGFace model for image feature extraction\nimage_input = Input(shape=(desired_width, desired_height, num_channels))\nvgg_model = VGGFace(model='vgg16', include_top=False, input_tensor=image_input)\n\n# Freeze layers to retain pretrained weights\nfor layer in vgg_model.layers:\n    layer.trainable = False\n\nx = Flatten()(vgg_model.output)\nimage_output = Dense(128, activation='relu')(x)\n\n# Build model for tabular features\ntabular_input = Input(shape=(X_tabular_train.shape[1],))\ntabular_output = Dense(128, activation='relu')(tabular_input)\n\n# Step 3: Combine Models\nconcatenated = Concatenate(axis=-1)([image_output, tabular_output])\nx = Dense(64, activation='relu')(concatenated)\noutput = Dense(1, activation='linear')(x)\n\ncombined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n\n# Step 4: Train and Evaluate\ncombined_model.compile(optimizer='adam', loss='mean_squared_error')\ncombined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\ny_pred = combined_model.predict([X_images_test, X_tabular_test])\n\n# Calculate metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\n# Assuming you have binary classification labels for f1_score\ny_test_binary = (y_test > 0).astype(int)\ny_pred_binary = (y_pred > 0).astype(int)\nf1 = f1_score(y_test_binary, y_pred_binary)\n\nprint(f'Mean Absolute Error: {mae}')\nprint(f'Mean Squared Error: {mse}')\nprint(f'F1 Score: {f1}')\n","block_group":"a3f4a21b3f194bdd83bf4280af8fd4cd","execution_count":19,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cv2'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"]}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"5b9ec9a2acf8452dbdccc405f625379e","deepnote_cell_type":"text-cell-p"},"source":"Mean Absolute Error: 1.288718291123708\r\nMean Squared Error: 2.2314737071360065\r\nF1 Score: 0.8679245283018869","block_group":"bcf9cb2580ff47cdbd9c146bf494db88"},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"48335019686d4e4ebd7fd57c22d1337c","deepnote_cell_type":"text-cell-h1"},"source":"# streamlit","block_group":"c32ff178511c42c3bf12af7867e7ea0f"},{"cell_type":"code","metadata":{"source_hash":"af63607b","execution_start":1706732661303,"execution_millis":68,"deepnote_to_be_reexecuted":false,"cell_id":"5116ef52534c4f9d95ffc571a5ce8ad5","deepnote_cell_type":"code"},"source":"#building up the model\n\n\ndef load_and_preprocess_data():\n    # Load your data \n    zip_path = \"dataset/age.zip\"  \n    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n        zip_ref.extractall()\n    df = pd.read_csv('age_detection.csv')\n    image_path = df.loc[0, 'file']\n    if not os.path.isabs(image_path):\n        image_path = os.path.join(os.path.dirname(zip_path), image_path)\n    df = df[df['file'].apply(lambda x: os.path.isfile(x))]\n    # Duplicate Removal\n    df.drop_duplicates(subset=['file'], keep='first', inplace=True)\n    df = df[df['split'].isin(['train', 'test'])]    \n    # Image Quality\n    # removing images that are too small\n    min_width, min_height = 64, 64  # minimum acceptable dimensions\n    def is_image_large_enough(file_path):\n        with Image.open(file_path) as img:\n            return img.width >= min_width and img.height >= min_height\n    df = df[df['file'].apply(is_image_large_enough)]\n\n    # Image Preprocessing\n    # resizing and normalizing images\n    def preprocess_image(file_path):\n        with Image.open(file_path) as img:\n            img = img.resize((min_width, min_height))  # resize\n            img = img.convert('RGB')  # ensure 3 channels\n            img = np.array(img) / 255.0  # normalize to [0, 1]\n        return img\n    df['image'] = df['file'].apply(preprocess_image)\n    detector = dlib.get_frontal_face_detector()\n\n    def contains_face(file_path):\n        img = cv2.imread(file_path)\n        faces = detector(img, 1)\n        return len(faces) > 0\n\n    df = df[df['file'].apply(contains_face)]\n    predictor = dlib.shape_predictor('dlib/shape_predictor_68_face_landmarks.dat')  # Download this file\n\n    def align_face(file_path):\n        img = cv2.imread(file_path)\n        faces = detector(img, 1)\n        for rect in faces:\n            shape = predictor(img, rect)\n            aligned_face = dlib.get_face_chip(img, shape)\n            return aligned_face\n\n    df['aligned_face'] = df['file'].apply(align_face)\n\n    # Background Removal\n\n    # Lighting and Color Normalization\n    def normalize_image(face):\n        gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n        normalized = cv2.equalizeHist(gray)\n        return normalized\n\n    df['normalized_face'] = df['aligned_face'].apply(normalize_image)\n\n    # Data Augmentation\n    def augment_image(face):\n        M = cv2.getRotationMatrix2D((face.shape[1] / 2, face.shape[0] / 2), np.random.uniform(-30, 30), 1)\n        rotated = cv2.warpAffine(face, M, (face.shape[1], face.shape[0]))\n        return rotated\n\n    df['augmented_face'] = df['normalized_face'].apply(augment_image)\n    label_encoder = LabelEncoder()\n    df['ageLabel'] = label_encoder.fit_transform(df['age'])\n\n    # Create a new column in the DataFrame to store texture features\n    df['texture_features'] = None\n\n    # Function to extract basic texture features from an image\n    def get_texture_features(image_path):\n        # Load the image\n        image = cv2.imread(image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur to smooth the image\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Compute texture features using the Laplacian operator\n        laplacian = cv2.Laplacian(blurred, cv2.CV_64F)\n        laplacian_var = np.var(laplacian)\n\n        return laplacian_var\n\n    # Apply the get_texture_features function to each image in the DataFrame\n    texture_features_list = []\n    for image_path in tqdm(df['file']):\n        texture_features = get_texture_features(image_path)\n        texture_features_list.append(texture_features)\n\n    # Add the extracted texture features to the DataFrame\n    df['texture_features'] = texture_features_list\n\n    df['wrinkle_features'] = None\n\n    # Function to extract basic wrinkle features from an image\n    def get_wrinkle_features(image_path):\n        # Load the image\n        image = cv2.imread(image_path)\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Canny edge detection to enhance wrinkles\n        edges = cv2.Canny(gray, 50, 150)\n\n        # Compute the percentage of white pixels in the edges\n        wrinkle_percentage = np.sum(edges) / (gray.shape[0] * gray.shape[1])\n\n        return wrinkle_percentage\n\n    # Apply the get_wrinkle_features function to each image in the DataFrame\n    wrinkle_features_list = []\n    for image_path in tqdm(df['file']):\n        wrinkle_features = get_wrinkle_features(image_path)\n        wrinkle_features_list.append(wrinkle_features)\n\n    # Add the extracted wrinkle features to the DataFrame\n    df['wrinkle_features'] = wrinkle_features_list\n\n    # Load the pre-trained facial landmark predictor\n    predictor_path = \"dlib/shape_predictor_68_face_landmarks.dat\"  # Replace with the path to the shape predictor model\n    detector = dlib.get_frontal_face_detector()\n    predictor = dlib.shape_predictor(predictor_path)\n\n    # Function to detect facial landmarks and extract eye-related features\n    def detect_eye_features(image_pixels):\n        # Convert the list of pixel values to a NumPy array\n        image_array = np.array(image_pixels, dtype=np.uint8)\n\n        # Ensure the image has 3 channels (for compatibility with cv2.COLOR_BGR2GRAY)\n        if image_array.ndim == 2:\n            image_array = cv2.cvtColor(image_array, cv2.COLOR_GRAY2BGR)\n\n        # Convert the image to grayscale\n        gray = cv2.cvtColor(image_array, cv2.COLOR_BGR2GRAY)\n\n        # Detect faces in the image\n        faces = detector(gray)\n\n        # Check if a face is detected\n        if len(faces) > 0:\n            # Get facial landmarks for the first detected face\n            shape = predictor(gray, faces[0])\n\n            # Extract eye-related features\n            left_eye_openness = shape.part(47).y - shape.part(43).y  # Example: vertical distance between eyebrow and lower eyelid\n            right_eye_openness = shape.part(40).y - shape.part(38).y\n\n            return left_eye_openness, right_eye_openness\n\n    # Example: Apply the detect_eye_features function to each image in the DataFrame\n    eye_features = df['normalized_face'].apply(detect_eye_features)\n\n    # Example: Add the extracted eye features to the DataFrame\n    df[['left_eye_openness', 'right_eye_openness']] = pd.DataFrame(eye_features.tolist(), index=df.index)\n    df = df.dropna()\n\n    return df\n\ndef load_model(df_cleaned):\n    # Load the trained model\n    desired_width = 128\n    desired_height = 128\n    num_channels = 3  # Assuming color images, adjust if grayscale\n\n    # Step 1: Prepare Data\n    # Extract 'eye openness', 'texture features', and 'wrinkle features'\n    tabular_features = df_cleaned[['texture_features', 'wrinkle_features', 'left_eye_openness','right_eye_openness']]\n    # Standardize numerical features\n    scaler = StandardScaler()\n    tabular_features = scaler.fit_transform(tabular_features)\n\n    # Load and preprocess images\n    desired_channels = 3  # Assuming color images, adjust if grayscale\n\n    def preprocess_image(image):\n        # Resize to a common size\n        resized_image = cv2.resize(image, (desired_width, desired_height))\n        # Ensure the image has the correct number of channels\n        if resized_image.shape[-1] != desired_channels:\n            # If the image has more than 3 channels, convert to grayscale\n            resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n            # Add an extra dimension to represent the single channel\n            resized_image = np.expand_dims(resized_image, axis=-1)\n        # Normalize pixel values to be between 0 and 1\n        resized_image = resized_image / 255.0\n        # Add other preprocessing steps as needed (e.g., data augmentation)\n        return resized_image\n\n    X_images = np.array([preprocess_image(image) for image in df_cleaned['image'].values])\n    # Extract target\n    y_target = df_cleaned['ageLabel'].values\n\n    # Split data into training and testing sets\n    X_images_train, X_images_test, X_tabular_train, X_tabular_test, y_train, y_test = train_test_split(\n        X_images, tabular_features, y_target, test_size=0.2, random_state=42\n    )\n\n    # Step 2: Build Models\n    # Build CNN model for image feature extraction\n    image_input = Input(shape=(desired_width, desired_height, num_channels))\n    x = Conv2D(32, (3, 3), activation='relu')(image_input)\n    x = MaxPooling2D((2, 2))(x)\n    x = Conv2D(64, (3, 3), activation='relu')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Flatten()(x)\n    image_output = Dense(128, activation='relu')(x)\n\n    # Build model for tabular features\n    tabular_input = Input(shape=(X_tabular_train.shape[1],))\n    tabular_output = Dense(128, activation='relu')(tabular_input)\n\n    # Step 3: Combine Models\n    concatenated = Concatenate(axis=-1)([image_output, tabular_output])\n    x = Dense(64, activation='relu')(concatenated)\n    output = Dense(1, activation='linear')(x)\n\n    combined_model = Model(inputs=[image_input, tabular_input], outputs=output)\n\n    # Step 4: Train and Evaluate\n    combined_model.compile(optimizer='adam', loss='mean_squared_error')\n    combined_model.fit([X_images_train, X_tabular_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n    # Evaluate the model\n    y_pred = combined_model.predict([X_images_test, X_tabular_test])\n\n    return combined_model\n\ndef preprocess_input(user_input):\n    # Assuming user_input is a preprocessed image (e.g., aligned and normalized)\n    processed_input = user_input.resize((desired_width, desired_height))  # Resize if needed\n    processed_input = np.array(processed_input) / 255.0  # Normalize to [0, 1]\n    processed_input = np.expand_dims(processed_input, axis=0)  # Add batch dimension\n    return processed_input\n\ndef make_prediction(model, processed_input):\n    # Assuming processed_input is a tuple containing (image_input, tabular_input)\n    image_input, tabular_input = processed_input\n\n    # Make predictions using the model\n    prediction = model.predict([image_input, tabular_input])\n\n    return prediction","block_group":"c6bad9930bf647fba56d4928127195bb","execution_count":20,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'cv2'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [20], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mzipfile\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdlib\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"]}]},{"cell_type":"code","metadata":{"source_hash":"3eab12df","execution_start":1706732665186,"execution_millis":20,"deepnote_to_be_reexecuted":false,"cell_id":"5996e3c87d604fc9b182214b81bf8ac7","deepnote_cell_type":"code"},"source":"# Save the trained model\nage_predicton = load_model(load_and_preprocess_data())\nage_predicton.save('saved_model.h5')","block_group":"b1b97378acf5496a8ac0f8b16556bcd8","execution_count":21,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'load_and_preprocess_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m age_predicton \u001b[38;5;241m=\u001b[39m load_model(\u001b[43mload_and_preprocess_data\u001b[49m())\n\u001b[1;32m      3\u001b[0m age_predicton\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'load_and_preprocess_data' is not defined"]}]},{"cell_type":"code","metadata":{"source_hash":"e7a689e8","execution_start":1706732666775,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"7f83c8ba99cc448b9a01cf7ad224fd3a","deepnote_cell_type":"code"},"source":"# model.py\n\nfrom keras.models import load_model\n\ndef load_age_model():\n    \n    # Load the trained model\n    model = load_model('saved_model.h5')  # Adjust the filename based on your saved model\n    return model","block_group":"b1641699433649919a989ac7a5908a7e","execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"e1876c93","execution_start":1706732669000,"execution_millis":21,"deepnote_to_be_reexecuted":false,"cell_id":"fd8dbe333be1490880be6b619eed96c7","deepnote_cell_type":"code"},"source":"# streamlit_app.py\n\nimport streamlit as st\nfrom PIL import Image\nimport numpy as np\n\n# Load the model\nmodel = load_age_model()\n\ndef main():\n    st.title(\"Age Detection App\")\n\n    # File uploader for image input\n    uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n\n    if uploaded_file is not None:\n        # Display the uploaded image\n        image = Image.open(uploaded_file)\n        st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n\n        # Resize the image to match the model's expected input size\n        resized_image = image.resize((128, 128))  # Adjust dimensions based on your model\n        st.image(resized_image, caption=\"Resized Image\", use_column_width=True)\n\n        # Preprocess the input\n        processed_input = preprocess_input(resized_image)\n\n        # Make a prediction\n        prediction = make_prediction(model, processed_input)\n\n        # Display the result\n        # Display the result\n        st.write(f\"Predicted Age: {model.prediction[0]}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","block_group":"f46b4e7469f34ef9b08abd5300b7f576","execution_count":23,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'streamlit'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# streamlit_app.py\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"]}]},{"cell_type":"code","metadata":{"cell_id":"06119f4126764d8cae63b54b4a5260d8","deepnote_cell_type":"code"},"source":"","block_group":"e82a75b9109d48218fb86645557d7ede","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"a75fa6a687464a318d5820fabe694ed4","deepnote_cell_type":"code"},"source":"","block_group":"301deec66a7c4cd385cd28fafa70f5c6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":"b623e53d","execution_start":1706732601705,"execution_millis":18,"deepnote_to_be_reexecuted":false,"cell_id":"fe80aeca33024fcfb62571827b28cd19","deepnote_cell_type":"code"},"source":"","block_group":"857b77158e6d4946bdd2eeb1a7d6759e","execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"70bec0416e3749c2bf12cab9c53678a2","deepnote_cell_type":"code"},"source":"","block_group":"e81e056db8a547709837eec5715ac0f3","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d3193c3-f84c-4987-b2bb-da138af83f70' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_persisted_session":{"createdAt":"2024-01-28T20:08:01.801Z"},"deepnote_notebook_id":"da34034b88264b5aa34ff590634af59d","deepnote_execution_queue":[]}}